{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud Constable Content-Based Spam/Fraud Detection\n",
    "______\n",
    "### Stephen Camera-Murray, Himani Garg, Vijay Thangella\n",
    "## CSDMC2010 SPAM Corpus\n",
    "(http://csmining.org/index.php/spam-email-datasets-.html)\n",
    "\n",
    "4327 messages out of which there are 2949 non-spam messages (HAM) and 1378 spam messages (SPAM)\n",
    "\n",
    "Spam                                                   |  Ham\n",
    ":-----------------------------------------------------:|:------------------------------------------------------:\n",
    "<img src=\"Spam.png\" alt=\"Spam\" style=\"width: 200px;\"/> | <img src=\"Ham.png\" alt=\"Ham\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Operationalize the Model\n",
    "____\n",
    "In this step, we'll build a function that accepts the email contents as a string and: 1) cleanses the text, 2) vectorizes the words, 3) counts the URL links, and 4) returns a \"suspicious\" probablity score along with a \"phishy\" flag.\n",
    "\n",
    "**Note**: A lot of preprocessing goes in to producing a single score, including unpickling our model. In order to do this in real-time, we'd likely need a good amount of tuning or more likely a way to keep the vectorizer and model \"warm\" while waiting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, pickle\n",
    "#import seaborn as sns\n",
    "#import itertools\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#from sklearn import metrics\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "from sklearn.externals import joblib\n",
    "import email.parser\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Email text cleaner function\n",
    "This function cleans up our email text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# email_text_cleaner ( email_text ) - function to cleanse email text\n",
    "#   parameters:\n",
    "#     email_text - the entire text of the email\n",
    "#   returns:\n",
    "#     clean_text   - the cleansed email subject and body\n",
    "#     num_of_links - count of links in the email\n",
    "#\n",
    "#   example: clean_text, num_of_links = email_text_cleaner ( email_text )\n",
    "#\n",
    "def email_text_cleaner ( email_text ):\n",
    "\n",
    "    # extract subject and body from the email text\n",
    "    msg = email.message_from_string ( email_text )\n",
    "    payload = msg.get_payload()\n",
    "    if type(payload) == type(list()) :\n",
    "        payload = payload[0] # only use the first part of payload\n",
    "    sub = msg.get('subject')\n",
    "    sub = str(sub)\n",
    "    if type(payload) != type('') :\n",
    "        payload = str(payload)\n",
    "\n",
    "    # concat the subject and body\n",
    "    email_content = sub + payload\n",
    "\n",
    "    # load the content into beautiful soup to parse the html, if any\n",
    "    soup = BeautifulSoup(email_content, \"lxml\")\n",
    "        \n",
    "    # Get the number of links-- maybe use to see if email is phishy\n",
    "    if ( soup.a is not None ):\n",
    "        num_of_links = len ( soup.a )\n",
    "    else:\n",
    "        num_of_links = 0\n",
    "\n",
    "    # remove script and style elements, may not be necessary for emails\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()\n",
    "\n",
    "    # extract the text\n",
    "    text = soup.get_text()\n",
    "        \n",
    "    # regex to replace non-letters with blank. Note: we also want to remove\n",
    "    # the phrase \"[SPAM]\" which is added by some the mailer's spam-detection\n",
    "    # so we're not cheating :-)\n",
    "    clean_text = re.sub(\"\\[SPAM\\]|[^a-zA-Z]\", \" \", text ).lower()\n",
    "    \n",
    "    return clean_text, num_of_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Email scoring function\n",
    "This function scores our email text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# score_email ( email_text ) - function to score email text\n",
    "#   parameters:\n",
    "#     email_text - the entire text of the email\n",
    "#   returns:\n",
    "#     score  - the probability score that the email is suspicious\n",
    "#     phishy - 1 for phishy, 0 for not phishy\n",
    "#\n",
    "#   example: score, phishy = score_email ( email_text )\n",
    "#\n",
    "def score_email ( email_text ):\n",
    "    \n",
    "    # unpickle our dictionary and model\n",
    "    with open ( \"dictionary.pkl\", \"rb\" ) as fp:\n",
    "        dict = pickle.load ( fp )\n",
    "\n",
    "    model = joblib.load ( 'SpamClassificationModel.pkl' )\n",
    "    \n",
    "    # cleanse our email content\n",
    "    clean_text, num_of_links = email_text_cleaner ( email_text )\n",
    "\n",
    "    # set up the vectorizer with our dictionary\n",
    "    vectorizer = CountVectorizer(stop_words='english', vocabulary=dict)\n",
    "    emailsVec = vectorizer.fit_transform([clean_text])\n",
    "    \n",
    "    # if the email contains more than one link, flag it as \"phishy\"\n",
    "    phishy = np.sign(num_of_links)\n",
    "    \n",
    "    # clean up our features set into a tidy dataframe (note: be sure to preserve the column order!)\n",
    "    linkDF = pd.DataFrame( {\"link_count\": [num_of_links], \"phishy\": [phishy]})\n",
    "    wordCounts = pd.DataFrame(emailsVec.toarray(), columns=dict) # convert vectors to a dataframe\n",
    "    emailFeatures = pd.concat([linkDF[[\"phishy\",\"link_count\"]], wordCounts], axis=1) # append the vectors to the labels\n",
    "\n",
    "    suspicious_score = model.predict_proba ( emailFeatures ) [:,0] [0]\n",
    "    \n",
    "    return suspicious_score, phishy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example run\n",
    "This test run simulates real-time prediction with our model. For our example text, we'll use the email text from spam_example.txt and ham_example.txt. In real life, we would have the email's text in-flight and call the scoring function directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spam example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted probability that this email is suspicious is 99.0 % and we believe it is phishy\n"
     ]
    }
   ],
   "source": [
    "# read the email file\n",
    "with open('data/spam_example.txt', 'r') as myfile:\n",
    "    email_text = myfile.read()\n",
    "\n",
    "# get our scores\n",
    "suspicious_score, phishy = score_email ( email_text )\n",
    "\n",
    "if ( phishy ):\n",
    "    phishy_ind = 'phishy'\n",
    "else:\n",
    "    phishy_ind = 'not phishy'\n",
    "\n",
    "print ( \"The predicted probability that this email is suspicious is\", ( suspicious_score * 100 ), \"% and we believe it is\", phishy_ind )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ham example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted probability that this email is suspicious is 5.0 % and we believe it is not phishy\n"
     ]
    }
   ],
   "source": [
    "# read the email file\n",
    "with open('data/ham_example.txt', 'r') as myfile:\n",
    "    email_text = myfile.read()\n",
    "\n",
    "# get our scores\n",
    "suspicious_score, phishy = score_email ( email_text )\n",
    "\n",
    "if ( phishy ):\n",
    "    phishy_ind = 'phishy'\n",
    "else:\n",
    "    phishy_ind = 'not phishy'\n",
    "\n",
    "print ( \"The predicted probability that this email is suspicious is\", ( suspicious_score * 100 ), \"% and we believe it is\", phishy_ind )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
