{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud Constable Content-Based Spam/Fraud Detection\n",
    "______\n",
    "### Stephen Camera-Murray, Himani Garg, Vijay Thangella\n",
    "## CSDMC2010 SPAM corpus\n",
    "(http://csmining.org/index.php/spam-email-datasets-.html)\n",
    "\n",
    "4327 messages out of which there are 2949 non-spam messages (HAM) and 1378 spam messages (SPAM)\n",
    "\n",
    "Spam                                                   |  Ham\n",
    ":-----------------------------------------------------:|:------------------------------------------------------:\n",
    "<img src=\"Spam.png\" alt=\"Spam\" style=\"width: 200px;\"/> | <img src=\"Ham.png\" alt=\"Ham\" style=\"width: 200px;\"/>\n",
    "\n",
    "**Note**: Emails are zipped using 7-Zip in data/emails.7z. In order to run this notebook, be sure to unzip the files in place. Be sure to delete all email files *before* committing changes to github so they don't get angry with us. :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Data Exploration and Cleansing\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import email.parser \n",
    "import os, sys, stat\n",
    "import shutil\n",
    "import nltk\n",
    "from PIL import Image\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create helper function\n",
    "Each email is separate, as are the labels defining the email as spam (0) or ham (1). We modify the functions included with the dataset to parse the files and strip out the email subject and body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ExtractSubPayload (filename):\n",
    "    ''' Extract the subject and payload from the .eml file.\n",
    "\n",
    "    '''\n",
    "    fp = open(filename, encoding=\"Latin-1\") # other encodings produced errors. while not technically correct, as long\n",
    "                                            # as the character is garbled in the same way all is well\n",
    "    msg = email.message_from_file(fp)\n",
    "    payload = msg.get_payload()\n",
    "    if type(payload) == type(list()) :\n",
    "        payload = payload[0] # only use the first part of payload\n",
    "    sub = msg.get('subject')\n",
    "    sub = str(sub)\n",
    "    if type(payload) != type('') :\n",
    "        payload = str(payload)\n",
    "\n",
    "    return sub + payload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the spam labels\n",
    "We load the labels into a dataframe and peek at the first five rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>TRAIN_00000.eml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>TRAIN_00001.eml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>TRAIN_00002.eml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>TRAIN_00003.eml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>TRAIN_00004.eml</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ham         filename\n",
       "0    0  TRAIN_00000.eml\n",
       "1    0  TRAIN_00001.eml\n",
       "2    1  TRAIN_00002.eml\n",
       "3    0  TRAIN_00003.eml\n",
       "4    0  TRAIN_00004.eml"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the labels file\n",
    "spamLabels = pd.read_csv (\n",
    "    'data/SPAMTrain.label',\n",
    "    sep=' ',\n",
    "    header=None,\n",
    "    names=['ham','filename']\n",
    ")\n",
    "spamLabels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop through, process email files, and add them to a single dataframe\n",
    "We loop through our directory containing the email dataset and:\n",
    "1. Extract the subject and message body\n",
    "2. Strip out html tags with beautifulsoup\n",
    "3. Count the number of links (to start, we're calling non-ham emails with at least one link phishy)\n",
    "4. Match it up with the correct label\n",
    "5. Append it to our full dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create an empty dataframe for the emails\n",
    "emailsDF = pd.DataFrame()\n",
    "\n",
    "# loop through each file in the directory, skipping the label file\n",
    "srcdir = 'data'\n",
    "files = os.listdir(srcdir)\n",
    "for idx, file in enumerate(files):\n",
    "    \n",
    "    srcpath = os.path.join(srcdir, file)\n",
    "    if ( '.eml' in file ):\n",
    "\n",
    "        # extract the subject and body\n",
    "        body = ExtractSubPayload (srcpath)\n",
    "\n",
    "        # load the body into beautiful soup to parse the html, if any\n",
    "        soup = BeautifulSoup(body, \"lxml\")\n",
    "        \n",
    "        # Get the number of links-- maybe use to see if email is phishy\n",
    "        if ( soup.a is not None ):\n",
    "            num_of_links = len ( soup.a )\n",
    "        else:\n",
    "            num_of_links = 0\n",
    "\n",
    "        # remove script and style elements, may not be necessary for emails\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "\n",
    "        # extract the text\n",
    "        text = soup.get_text()\n",
    "        \n",
    "        # regex to replace non-letters with blank. Note: we also want to remove\n",
    "        # the phrase \"[SPAM]\" which is added by some the mailer's spam-detection\n",
    "        # so we're not cheating :-)\n",
    "        clean_text = re.sub(\"\\[SPAM\\]|[^a-zA-Z]\", \" \", text ).lower()\n",
    "        \n",
    "        # append label, link_counts, phishy flag, and email body to our dataframe\n",
    "        emailsDF = emailsDF.append({'ham':spamLabels[spamLabels['filename']==file]['ham'].values[0],\n",
    "                                    'phishy':np.sign((1-spamLabels[spamLabels['filename']==file]['ham'].values[0])*num_of_links),\n",
    "                                    'link_count':num_of_links, 'content':clean_text, }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>ham</th>\n",
       "      <th>link_count</th>\n",
       "      <th>phishy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of a kind money maker  try it for free con...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>link to my webcam you wanted wanna see sexuall...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>re  how to manage multiple internet connection...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>give her   hour rodeoenhance your desire  p...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>best price on the netf f m   suddenlysusan sto...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  ham  link_count  phishy\n",
       "0  one of a kind money maker  try it for free con...  0.0         1.0     1.0\n",
       "1  link to my webcam you wanted wanna see sexuall...  0.0         0.0     0.0\n",
       "2  re  how to manage multiple internet connection...  1.0         0.0     0.0\n",
       "3     give her   hour rodeoenhance your desire  p...  0.0         0.0     0.0\n",
       "4  best price on the netf f m   suddenlysusan sto...  0.0         0.0     0.0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emailsDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Word Clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pull all of the words into two single strings, one for spam and one for ham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get all of the spam words and ham words in a single string\n",
    "spam_words = emailsDF[emailsDF['ham']==0]['content'].str.cat()\n",
    "ham_words  = emailsDF[emailsDF['ham']==1]['content'].str.cat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the word cloud images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x111e1930dd8>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = os.path.dirname('.')\n",
    "\n",
    "spam_mask = np.array(Image.open(os.path.join(d, \"Spam.jpg\")))\n",
    "ham_mask = np.array(Image.open(os.path.join(d, \"Ham.jpg\")))\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "# generate word cloud\n",
    "wc_spam = WordCloud(background_color=None, mode=\"RGBA\", max_words=100, mask=spam_mask,\n",
    "               stopwords=stopwords)\n",
    "wc_spam.generate(spam_words)\n",
    "\n",
    "wc_ham = WordCloud(background_color=None, mode=\"RGBA\", max_words=100, mask=ham_mask,\n",
    "               stopwords=stopwords)\n",
    "wc_ham.generate(ham_words)\n",
    "\n",
    "# store to file\n",
    "wc_spam.to_file(os.path.join(d, \"SpamWordCloud.png\"))\n",
    "wc_ham.to_file (os.path.join(d, \"HamWordCloud.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that each set of classified words are quite different and should be useful in building a predictive model. There is some repetition, but that is likely due to extra spaces that were not filtered out in our cleansing. Tokenization in our data preparation step should take care of this, but we should double-check to be sure. One interesting detail we also notice is the ham-classified dataset seems to be tech-heavy, which may not be representative. Once we build the model, we should use additional datasets to check the accuracy of our final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spam                                                   |  Ham\n",
    ":-----------------------------------------------------:|:------------------------------------------------------:\n",
    "<img src=\"SpamWordCloud.png\" alt=\"Spam\" style=\"height: 300px;\"/> | <img src=\"HamWordCloud.png\" alt=\"Ham\" style=\"height: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write our cleansed dataset to the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write the cleansed dataframe to a file\n",
    "emailsDF.to_csv('data/cleansedEmails.tab.gz', index=False, compression='gzip', sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
